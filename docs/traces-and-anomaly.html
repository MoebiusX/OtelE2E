<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Traces & Anomaly Monitoring ‚Äî A Practical Guide</title>
<link rel="stylesheet" href="styles/doc.css" />
</head>
<body>
  <div class="article-wrapper">
    <header class="article-header">
      <h1>Traces & Anomaly Monitoring ‚Äî A Practical Guide</h1>
      <p class="lede">A compact, Medium-style walkthrough that explains the tracing decisions we made and the design/implementation of the time-aware + adaptive anomaly detection system.</p>
    </header>

    <main id="content"><p>A compact, Medium-style walkthrough that explains the tracing decisions we made and the design/implementation of the time-aware + adaptive anomaly detection system. This is written to help you replicate the setup, run it in your environment, and extend it safely.</p>
<hr>
<h2>TL;DR üöÄ</h2>
<ul>
<li>We fixed span parent/child propagation across browser ‚Üí gateway ‚Üí API ‚Üí RabbitMQ ‚Üí worker by explicitly setting and passing contexts and by injecting a known <code>x-parent-traceparent</code> header through the queue.</li>
<li>We built a monitoring pipeline that computes <strong>time-aware baselines</strong> (1-hour buckets, 30-day window) and derives <strong>adaptive thresholds</strong> (percentiles) nightly, mapping deviations to <strong>SEV1‚ÄìSEV5</strong>.</li>
<li>Ollama LLM is integrated to analyze traces + correlated Prometheus metrics; use the smaller <code>llama3.2:1b</code> for better latency in production-like environments.</li>
</ul>
<hr>
<p><img src="images/trace-hierarchy.png" alt="Trace Hierarchy"></p>
<p><em>Figure: Trace flow and context propagation (Browser ‚Üí Gateway ‚Üí API ‚Üí RabbitMQ ‚Üí Worker).</em> </p>
<p><img src="images/baseline-calculation.png" alt="Baseline Calculation"></p>
<p><em>Figure: Nightly baseline calculation and real-time detection pipeline.</em></p>
<hr>
<h2>Key achievements ‚úÖ</h2>
<ul>
<li><strong>End-to-end tracing corrected and unified:</strong><ul>
<li>Proper parent/child span propagation for browser ‚Üí <code>api-gateway</code> ‚Üí <code>exchange-api</code> ‚Üí RabbitMQ ‚Üí <code>order-matcher</code> (fixed in <code>trade-form.tsx</code>, <code>rabbitmq-client.ts</code>, <code>index.ts</code>).</li>
</ul>
</li>
<li><strong>Trace hierarchy &amp; message correlation:</strong><ul>
<li>Injected and passed original parent context via message headers (<code>x-parent-traceparent</code>) so response processing appears as a sibling span.</li>
</ul>
</li>
<li><strong>Monitoring system built:</strong><ul>
<li>Baseline profiler, anomaly detector, history store and manual endpoint to <code>POST /api/monitor/recalculate</code>.</li>
<li>Time-aware baselines (1-hour buckets, 30-day history) and adaptive, percentile-based thresholds turned into five severity levels (SEV1‚ÄìSEV5).</li>
</ul>
</li>
<li><strong>LLM-augmented analysis:</strong><ul>
<li>Ollama integrated for AI-assisted trace+metrics analysis; prompts include correlated Prometheus metrics (cpu, mem, P99, etc.).</li>
<li>Switched to <code>llama3.2:1b</code> for faster responses.</li>
</ul>
</li>
<li><strong>Metrics + traces correlation:</strong><ul>
<li>Prometheus metrics collection and a metrics correlator service to fetch metrics at anomaly timestamps and include them in AI prompts.</li>
</ul>
</li>
<li><strong>UI &amp; UX improvements:</strong><ul>
<li>Monitor dashboard: severity filter dropdown, SEV badges and colors, AI Analysis moved above baseline table, visible header buttons, accessibility/contrast and layout refinements.</li>
</ul>
</li>
<li><strong>Infra &amp; ops cleanup:</strong><ul>
<li>Docker compose fixes (port conflicts resolved), removed unused Tempo/Grafana, and added Prometheus scrape for <code>/metrics</code>.</li>
</ul>
</li>
</ul>
<hr>
<h2>Impact / measurable results üìä</h2>
<ul>
<li>Baselines collected (example): ~22 baselines from ~2.9k spans during initial run.</li>
<li>Tracing now shows deeper, correct hierarchy (e.g., <code>exchange-api:POST</code> ‚Üí <code>publish orders</code> and sibling <code>payment_response process</code>).</li>
<li>AI responses became data-driven after including real metrics (reduced hallucination).</li>
</ul>
<hr>
<h2>Lessons learned &amp; best practices üí°</h2>
<ul>
<li><strong>Explicit context propagation matters:</strong> always set parent context explicitly around async operations (use <code>context.with</code> / <code>trace.setSpan</code>), especially across await boundaries.</li>
<li><strong>Message brokers require intentional header propagation:</strong> to maintain meaningful trace relationships across queue-based patterns, send the original parent trace context explicitly.</li>
<li><strong>Time-awareness is essential:</strong> 1-hour time buckets with 30-day history dramatically reduce false positives compared to single global baselines.</li>
<li><strong>Adaptive thresholds beat fixed œÉ:</strong> calculating percentile breakpoints (p95/p99/p99.9) nightly yields meaningful SEV boundaries per-span.</li>
<li><strong>LLMs must be fed precise, validated context:</strong> include raw metrics in prompts and log the prompt payload to detect hallucination quickly.</li>
<li><strong>UX affects adoption:</strong> place analysis near selection (AI Analysis above baselines), show severity badges, and ensure visible controls and contrast.</li>
<li><strong>Incremental &amp; test-first rollout:</strong> start with polling-based detection, a manual recalculation endpoint, and iterate to nightly automation and streaming later.</li>
<li><strong>Performance tradeoffs:</strong> smaller LLM models and quantized variants give big latency/throughput wins; keep a tiered/streaming plan for heavy usage.</li>
<li><strong>Keep debug logs brief during validation:</strong> remove verbose logs after verifying behavior.</li>
<li><strong>Document tracing touchpoints &amp; config:</strong> include clear documentation so others can reproduce the same trace semantics.</li>
</ul>
<hr>
<h2>Part 1 ‚Äî How to replicate the tracing behavior ‚ú®</h2>
<p>Goal: achieve the same, clear trace hierarchy and reliable sibling/child relationships for operations that cross HTTP and message queues.</p>
<h3>Key concepts (short)</h3>
<ul>
<li>Context is not magically preserved across async boundaries ‚Äî explicitly preserve and use it.</li>
<li>When sending messages across message brokers, <strong>inject</strong> a trace context into message headers so the consumer can link spans correctly.</li>
<li>For response messages that should appear as siblings (not children) of an original request, <strong>carry the original parent span context</strong> and use that as the parent when creating the response consumer span.</li>
</ul>
<h3>Touchpoints and where the logic lives in the repo</h3>
<ul>
<li>Client-side: <code>client/src/lib/tracing.ts</code> and components like <code>client/src/components/trade-form.tsx</code> ‚Äî create client spans and preserve context across await boundaries when processing responses.</li>
<li>RabbitMQ client (producer): <code>server/services/rabbitmq-client.ts</code> ‚Äî create publish spans with the correct parent context and inject trace headers into message properties (e.g. <code>headers[&#39;traceparent&#39;]</code> or a custom <code>x-parent-traceparent</code>).</li>
<li>Worker / matcher (consumer): <code>payment-processor/index.ts</code> ‚Äî extract the parent context from message headers; when responding, inject the original POST context into the response so the consumer&#39;s processing span becomes a sibling of <code>publish orders</code>.</li>
<li>Auto-instrumentation: be mindful that amqplib/auto-instrumentation will create spans from extracted trace headers; decide whether to rely on it or implement explicit spans.</li>
</ul>
<h3>Minimal code patterns (Node / OpenTelemetry)</h3>
<ul>
<li>Create a span and run code in its context:</li>
</ul>
<pre><code class="language-js">const parentCtx = context.active();
const span = tracer.startSpan(&#39;order.submit&#39;);
await context.with(trace.setSpan(parentCtx, span), async () =&gt; {
  // your async code here (fetch, publish, etc.)
  // inject context for outgoing messages
  propagation.inject(context.active(), carrier, defaultTextMapSetter);
});
span.end();
</code></pre>
<ul>
<li>Inject parent trace into message headers (producer):</li>
</ul>
<pre><code class="language-js">const carrier = { headers: {} };
propagation.inject(context.active(), carrier.headers, defaultTextMapSetter);
// Also include original POST context explicitly as x-parent-traceparent
carrier.headers[&#39;x-parent-traceparent&#39;] = getTraceparentFromContext(context.active());
// publish message with &#39;carrier.headers&#39;
</code></pre>
<ul>
<li>Extract context on consumer and use the original POST context in responses:</li>
</ul>
<pre><code class="language-js">const headers = msg.properties.headers || {};
const parentCtx = propagation.extract(ROOT_CONTEXT, headers, defaultTextMapGetter);
const originalParent = headers[&#39;x-parent-traceparent&#39;];
// For response: set the original parent context so response span is sibling
const respCtx = reconstructContextFromTraceparent(originalParent);
const responseSpan = tracer.startSpan(&#39;payment_response process&#39;, { /* ...*/ }, respCtx);
</code></pre>
<blockquote>
<p>Tip: use consistent header naming (<code>x-parent-traceparent</code>) so you can find and debug propagation easily in logs.</p>
</blockquote>
<h3>Verification steps</h3>
<ol>
<li>Start the app and Jaeger: UI at <code>http://localhost:5173</code> and Jaeger at <code>http://localhost:16686</code>.</li>
<li>Submit a BUY order via UI or curl:</li>
</ol>
<pre><code class="language-bash">curl -X POST http://localhost:8000/api/orders -H &#39;Content-Type: application/json&#39; -d &#39;{&quot;pair&quot;:&quot;BTC/USD&quot;,&quot;side&quot;:&quot;BUY&quot;,&quot;quantity&quot;:0.01,&quot;orderType&quot;:&quot;MARKET&quot;,&quot;userId&quot;:&quot;alice&quot;}&#39;
</code></pre>
<ol start="3">
<li><p>In Jaeger you should see:</p>
<ul>
<li><code>exchange-api: POST</code> as root</li>
<li>Child <code>exchange-api: publish orders</code> ‚Üí down to <code>order-matcher: order.match</code></li>
<li>A sibling <code>exchange-api: payment_response process</code> that is a child of the original POST.</li>
</ul>
</li>
<li><p>If a span is missing or orphaned, check the message headers printed in logs (we added debug logs in <code>server/services/rabbitmq-client.ts</code> and <code>payment-processor/index.ts</code>).</p>
</li>
</ol>
<hr>
<h2>Part 2 ‚Äî Anomaly Detection Design (Time-aware + Adaptive thresholds) üß≠</h2>
<p>This section explains the architecture we implemented, why we chose it, and how to reproduce or adapt it to your data.</p>
<h3>Motivation</h3>
<ul>
<li>Latency and span durations vary by time-of-day and day-of-week.</li>
<li>A single global mean/std-dev produces many false positives.</li>
<li>We need a system that (1) captures time patterns, (2) learns what constitutes ‚Äúrare‚Äù deviations, and (3) exposes severity tiers for alerting.</li>
</ul>
<h3>High-level architecture</h3>
<ol>
<li>TraceProfiler polls Jaeger and stores span duration samples (per span operation) for the last 30 days.</li>
<li>BaselineCalculator groups samples into <strong>time buckets</strong> (dayOfWeek √ó hourOfDay) and computes incremental stats (count, sum, sum-of-squares ‚Üí mean/std).</li>
<li>Nightly job computes percentiles (p95, p99, p99.9) of historical deviations and derives severity thresholds (SEV1‚ÄìSEV5).</li>
<li>AnomalyDetector uses the best-available baseline (specific bucket ‚Üí hourly fallback ‚Üí global) and computes deviation in œÉ units: deviation = (value - mean) / œÉ.</li>
<li>If deviation exceeds severity thresholds, create an anomaly with associated metadata and store it in HistoryStore.</li>
<li>On anomaly selection, the AnalysisService fetches correlated Prometheus metrics and calls Ollama for insights.</li>
</ol>
<h3>Storage &amp; shapes (SQLite-style)</h3>
<pre><code>TimeBaseline { spanKey, dayOfWeek, hourOfDay, count, mean, stdDev }
Thresholds { spanKey, dayOfWeek, hourOfDay, p95, p99, p999, sevMapping }
Anomaly { id, spanKey, duration, deviationSigma, sev, traceId, timestamp }
</code></pre>
<h3>Key details &amp; decisions</h3>
<ul>
<li>Granularity: 1-hour buckets (24 √ó 7 = 168 per span)</li>
<li>History depth: 30 days (configurable)</li>
<li>Recalc mode: nightly job + manual endpoint <code>POST /api/monitor/recalculate</code> for validation and ad-hoc re-runs</li>
<li>Adaptive severity mapping: By default we map percentiles to SEV levels like this:<ul>
<li>SEV1: &gt; p999 (extreme, top 0.1%)</li>
<li>SEV2: &gt; p99 (critical)</li>
<li>SEV3: &gt; p95 (major)</li>
<li>SEV4: &gt; p90 (minor)</li>
<li>SEV5: &gt; p80 (low)</li>
</ul>
</li>
</ul>
<blockquote>
<p>This mapping is configurable and done during the nightly baseline recalculation.</p>
</blockquote>
<h3>Algorithm (nightly recalculation)</h3>
<ol>
<li>For each spanKey and each hour bucket:<ul>
<li>Query last 30 days of duration samples for that span &amp; bucket</li>
<li>Compute mean Œº, std œÉ, and percentiles of deviations</li>
<li>Store (mean, œÉ, p95, p99, p999)</li>
</ul>
</li>
<li>Create SEV threshold map from percentile results, persist for AnomalyDetector to use.</li>
</ol>
<h3>Real-time detection (per incoming span sample)</h3>
<ol>
<li>Identify bucket: dayOfWeek + hourOfDay of sample timestamp</li>
<li>Lookup baseline (if sampleCount &gt;= MIN_SAMPLES, use it; else fallback to hourly or global baseline)</li>
<li>Compute deviation: z = (duration - Œº) / œÉ</li>
<li>Map z to severity using stored thresholds</li>
<li>Create anomaly if z crosses configured SEV threshold</li>
</ol>
<h3>Example code sketch (detection)</h3>
<pre><code class="language-ts">const baseline = getBaseline(spanKey, day, hour) || fallbackBaseline;
const z = (observed - baseline.mean) / baseline.std;
const sev = classifyByThresholds(z, thresholdsFor(spanKey, day, hour));
if (sev &lt;= configuredAlertLevel) createAnomaly(...);
</code></pre>
<h3>Severity classification (SEV1‚ÄìSEV5)</h3>
<p>Make severity explicit in UI and storage and show the percentile and œÉ that triggered it. This allows users to filter and triage efficiently (we added a severity dropdown to the UI).</p>
<h3>Why not ML-first? (and when to use ML)</h3>
<ul>
<li>The incremental stats approach (mean/std/percentiles) is simple, explainable, and works with limited historical data.</li>
<li>ML methods (one-class SVM, isolation forest, autoencoder, or LoRA-finetuned models that summarize traces) can be applied later when you have labeled anomalies and richer feature vectors.</li>
<li>For now, prefer deterministic, inspectable thresholds with nightly recalculation and anomaly history.</li>
</ul>
<hr>
<h2>Part 3 ‚Äî LLM integration (operational tips)</h2>
<ul>
<li>Include correlated metrics (Prometheus) in the prompt when analyzing an anomaly ‚Äî CPU, memory, request rate, error rate, P95/P99 latency.</li>
<li>Keep prompts bounded (summarize long traces) and log prompt payloads during testing to avoid hallucinations.</li>
<li>Use a smaller model (we switched to <code>llama3.2:1b</code>) for production-like responsiveness; keep the larger model for ad-hoc deep analysis.</li>
<li>Add timeouts and retries to Ollama calls (we added both) and a circuit-breaker for model overload.</li>
</ul>
<p>Sample prompt structure:</p>
<pre><code>- Short summary of anomaly (service, span, duration, deviation)
- Recent trace (key spans and durations, up to a length limit)
- Correlated metrics snapshot (CPU, memory, P99, request-rate, error-rate)
- Ask the model for top-3 probable causes and recommended next steps
</code></pre>
<hr>
<h2>Part 4 ‚Äî Tests &amp; operational checklist ‚úÖ</h2>
<ul>
<li><input disabled="" type="checkbox"> Start services: <code>docker-compose up -d</code> and <code>npm run dev</code> (server + vite)</li>
<li><input disabled="" type="checkbox"> Verify Jaeger traces: Visit <code>http://localhost:16686</code> and search traces for <code>exchange-api</code> service</li>
<li><input disabled="" type="checkbox"> Submit order via UI or curl and verify: publish spans exist and response spans are siblings</li>
<li><input disabled="" type="checkbox"> Run baseline recalculation: <code>POST /api/monitor/recalculate</code> and check SEV thresholds stored</li>
<li><input disabled="" type="checkbox"> Trigger anomaly (simulate slow handler) and confirm UI receives an alert with SEV badge</li>
<li><input disabled="" type="checkbox"> Select anomaly ‚Üí click <code>Analyze</code> ‚Üí check model response mentions metrics and suggests actions</li>
</ul>
<hr>
<h2>Part 5 ‚Äî Next steps &amp; roadmap</h2>
<ul>
<li>Automate the nightly baseline job via cron/worker (server side) and alert on job failures.</li>
<li>Add a small finite buffer/summary extractor on trace content before sending to LLM (reduce prompt size and cost).</li>
<li>Create a labeled dataset and start LoRA fine-tuning for domain-specific insights (200‚Äì500 examples to start).</li>
<li>Add metric historical charts for anomalies and a service dependency map for root-cause suggestions.</li>
</ul>
<hr>
<h2>Appendix ‚Äî Quick commands and endpoints</h2>
<ul>
<li>Submit an order (example):<pre><code class="language-bash">curl -X POST http://localhost:8000/api/orders -H &#39;Content-Type: application/json&#39; -d &#39;{&quot;pair&quot;:&quot;BTC/USD&quot;,&quot;side&quot;:&quot;BUY&quot;,&quot;quantity&quot;:0.01,&quot;orderType&quot;:&quot;MARKET&quot;,&quot;userId&quot;:&quot;alice&quot;}&#39;
</code></pre>
</li>
<li>Recalculate baselines (manual):<pre><code class="language-bash">curl -X POST http://localhost:5000/api/monitor/recalculate -H &#39;Content-Type: application/json&#39;
</code></pre>
</li>
<li>Analyze an anomaly (trigger model):<pre><code class="language-bash">curl -X POST http://localhost:5000/api/monitor/analyze -H &#39;Content-Type: application/json&#39; -d &#39;{&quot;traceId&quot;:&quot;&lt;traceId&gt;&quot;}&#39;
</code></pre>
</li>
</ul>
<hr>
<h2>Export to HTML / PDF (one-command)</h2>
<p>To generate the rendered HTML and a printable PDF of this document (including diagrams):</p>
<pre><code class="language-bash"># Renders diagrams and builds HTML + PDF
npm run docs:build
</code></pre>
<p>Outputs:</p>
<ul>
<li><code>docs/traces-and-anomaly.html</code> (HTML)</li>
<li><code>docs/traces-and-anomaly.pdf</code> (PDF)</li>
</ul>
<hr>
<p>If you&#39;d like, I can now:</p>
<ul>
<li>Generate a short <code>docs/README_TRACING.md</code> version suitable for on-call staff,</li>
<li>Add a checklist <code>MONITORING_RUNBOOK.md</code> for incident responders,</li>
<li>Or create the Pull Request with these docs and update the main <code>README.md</code>.</li>
</ul>
<p>Which would you like me to do next? üôå</p>
</main>

    <footer class="footer">Generated from <code>docs/TRACES-AND-ANOMALY-MONITORING.md</code></footer>
  </div>
</body>
</html>